{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(\"..\")  # Adjust based on your folder structure\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agents.mcts.mcts_agent_negamax' from '/Users/alibal/Desktop/tactix-game/agents/mcts/mcts_agent_negamax.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agents.mcts.mcts_agent_negamax import MCTSAgent_negamax\n",
    "from agents.mcts.mcts_node import MCTSNode\n",
    "from tactix.utils import *\n",
    "from tactix.tactixEnvironment_without_opp import TactixEnvironment\n",
    "from tactix.tactixGame import TactixGame\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "importlib.reload(sys.modules['tactix.tactixGame'])\n",
    "importlib.reload(sys.modules['tactix.tactixEnvironment_without_opp'])\n",
    "importlib.reload(sys.modules['tactix.utils'])\n",
    "importlib.reload(sys.modules['agents.mcts.mcts_agent_negamax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, num_heads):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.hidden2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.output = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, input_dim)\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)  # Self-attention\n",
    "        x = self.relu(attn_output)      # Activation\n",
    "        x = self.relu(self.hidden1(x))  # Hidden layer 1\n",
    "        x = self.relu(self.hidden2(x))  # Hidden layer 2\n",
    "        x = self.output(x)              # Output layer\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 64\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 128\n",
    "output_dim = 20\n",
    "num_heads = 4       # or 1\n",
    "\n",
    "model = SimpleNN(input_dim, hidden_dim1, hidden_dim2, output_dim, num_heads)\n",
    "\n",
    "# Example input: batch_size=32, input_dim=64\n",
    "input_tensor = torch.randn(32, input_dim)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should be (32, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 125])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A DQN with a single attention layer after the input state.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.input_projection = nn.Linear(self.state_size, self.state_size * 3)\n",
    "        # Attention layer\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=state_size * 3, num_heads=3, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        input_dim = state_size * 3\n",
    "        for hidden_size in layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "        \n",
    "        # Final layer: from the last hidden layer to the action output\n",
    "        layers.append(nn.Linear(input_dim, action_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the DQN with an attention layer.\n",
    "        x: Input state tensor, shape [batch_size, state_size].\n",
    "        \"\"\"\n",
    "        # Pass through the input projection\n",
    "        x = self.input_projection(x)\n",
    "        # Add a sequence dimension: (batch_size, state_size) -> (batch_size, 1, state_size)\n",
    "        #x = x.unsqueeze(1)\n",
    "        \n",
    "        # Input shape: (batch_size, state_size)\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)  # Self-attention\n",
    "\n",
    "        # Remove the sequence dimension: (batch_size, 1, state_size) -> (batch_size, state_size)\n",
    "        #x = attn_output.squeeze(1)\n",
    "        \n",
    "        # Pass through the rest of the network\n",
    "        return self.network(x)  # Outputs Q-values for each action\n",
    "    \n",
    "model = DQN(state_size=25, action_size=125, layer_sizes=[128, 128])\n",
    "input_tensor = torch.randn(64, 25)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should be (64, 125)\n",
    "# print(x.shape)\n",
    "# x = x.squeeze(1)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 125])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HAS_ATTENTION = True\n",
    "\n",
    "class DQNWithDropoutAndResidual(nn.Module):\n",
    "    def __init__(self, state_size, action_size, num_hidden_layers, hidden_dim, dropout_prob=0.5):\n",
    "        super(DQNWithDropoutAndResidual, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.embedding_dim = hidden_dim\n",
    "\n",
    "        # Projection layer \n",
    "        self.projection =  nn.Linear(state_size, self.embedding_dim)\n",
    "\n",
    "        # Attention layer\n",
    "        if HAS_ATTENTION:\n",
    "            # Attention - Single or Multihead\n",
    "            num_heads = 4           # has to divide hidden_dim\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = [nn.Linear(self.embedding_dim, self.embedding_dim) for i in range(num_hidden_layers)]\n",
    "\n",
    "        # ReLu and Dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.embedding_dim, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Projection\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Attention\n",
    "        if HAS_ATTENTION:\n",
    "            x, _ = self.attention(x,x,x)\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        # Hidden layer 1 - no residual needed\n",
    "        h1 = self.relu(self.hidden_layers[0](x))\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Rest of the hidden layers\n",
    "        h = h1\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            h = self.relu(self.hidden_layers[i](h + x))\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "# Example instantiation\n",
    "model = DQNWithDropoutAndResidual(state_size=25, action_size=125, num_hidden_layers=3, hidden_dim=128, dropout_prob=0.5)\n",
    "input_tensor = torch.rand(64,25)\n",
    "output = model(input_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNWithDropoutAndResidual(\n",
      "  (projection): Linear(in_features=25, out_features=128, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (output_layer): Linear(in_features=128, out_features=125, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HAS_ATTENTION = True\n",
    "\n",
    "class DQNWithDropoutAndResidual(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(DQNWithDropoutAndResidual, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_hidden_layers = len(layer_sizes)\n",
    "        self.embedding_dim = layer_sizes[0]\n",
    "\n",
    "        # Projection layer \n",
    "        self.projection =  nn.Linear(state_size, self.embedding_dim)\n",
    "\n",
    "        # Attention layer\n",
    "        if HAS_ATTENTION:\n",
    "            # Attention - Single or Multihead\n",
    "            num_heads = 4           # has to divide hidden_dim\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.embedding_dim, self.embedding_dim) for i in range(self.num_hidden_layers)])\n",
    "\n",
    "        # Dropout probability\n",
    "        dropout_prob = 0.5\n",
    "\n",
    "        # ReLu and Dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.embedding_dim, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Projection\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Attention\n",
    "        if HAS_ATTENTION:\n",
    "            x, _ = self.attention(x,x,x)\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        # Hidden layer 1 - no residual needed\n",
    "        h1 = self.relu(self.hidden_layers[0](x))\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Rest of the hidden layers\n",
    "        h = h1\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            h = self.relu(self.hidden_layers[i](h + x))\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "# Example instantiation\n",
    "model = DQNWithDropoutAndResidual(state_size=25, action_size=125, layer_sizes=[128, 128, 128])\n",
    "input_tensor = torch.rand(64,25)\n",
    "output = model(input_tensor)\n",
    "output.shape\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Store a transition (s, a, r, s')\"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_size, \n",
    "        action_size, \n",
    "        layer_sizes,\n",
    "        lr=1e-3, \n",
    "        gamma=0.9, \n",
    "        epsilon_start=1.0, \n",
    "        epsilon_end=0.01, \n",
    "        epsilon_decay=0.999876,\n",
    "        memory_capacity=10000,\n",
    "        device='cpu',\n",
    "        pretrained_model_path = None\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "        \n",
    "        # Q-Networks (main + target)\n",
    "        self.q_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "\n",
    "        if pretrained_model_path:\n",
    "            self.q_network.load_state_dict(torch.load(pretrained_model_path, map_location=self.device))\n",
    "            print(f\"Loaded pretrained model from {pretrained_model_path}\")\n",
    "\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        # Replay Memory\n",
    "        self.memory = ReplayMemory(capacity=memory_capacity)\n",
    "\n",
    "    def select_action(self, state, valid_moves_mask):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection with invalid move masking.\n",
    "        state: shape (1, state_size)\n",
    "        valid_moves_mask: shape (1, action_size) -> 1/0 for valid moves\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            valid_indices = torch.where(valid_moves_mask[0] == 1)[0]\n",
    "            action = random.choice(valid_indices.tolist())\n",
    "            return action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state.to(self.device))  # (1, action_size)\n",
    "                # Mask invalid actions by setting them to -inf\n",
    "                q_values[valid_moves_mask == 0] = -float('inf')\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network to match the Q-network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    " \n",
    "    def train_step(self, batch_size):\n",
    "        \"\"\"Train the Q-network using one batch from experience replay.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Not enough samples to train\n",
    "        \n",
    "        # Sample a batch of transitions\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # transitions is a list of tuples: (state, action, reward, next_state, done)\n",
    "        batch = list(zip(*transitions))\n",
    "\n",
    "        states = torch.stack(batch[0]).to(self.device)          # shape: [batch_size, 1, state_size]\n",
    "        actions = torch.stack(batch[1]).to(self.device)         # shape: [batch_size]\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32).to(self.device)  # [batch_size]\n",
    "        next_states = torch.stack(batch[3]).to(self.device)     # shape: [batch_size, 1, state_size]\n",
    "        #next_states_valid_moves_mask = torch.stack(batch[4]).to(self.device)  # shape: [batch_size, 1, action_size]\n",
    "        dones = torch.tensor(batch[4], dtype=torch.bool).to(self.device)       # [batch_size]\n",
    "        \n",
    "        # Flatten states: we have [batch_size, 1, state_size] => [batch_size, state_size]\n",
    "        states = states.view(states.size(0), -1)\n",
    "        next_states = next_states.view(next_states.size(0), -1)\n",
    "\n",
    "        # Flatten next_states_valid_moves_mask: [batch_size, 1, action_size] => [batch_size, action_size]\n",
    "        #next_states_valid_moves_mask = next_states_valid_moves_mask.view(next_states_valid_moves_mask.size(0), -1)\n",
    "\n",
    "        # Current Q-values\n",
    "        q_values = self.q_network(states)\n",
    "        # Gather Q-values for the taken actions\n",
    "        # q_values shape is [batch_size, action_size], actions is [batch_size]\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():  \n",
    "            #temp_next_q_values = self.target_network(next_states)\n",
    "            #temp_next_q_values[next_states_valid_moves_mask == 0] = -float('inf')\n",
    "            #max_next_q_values = temp_next_q_values.max(1)[0]\n",
    "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones.float()) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # Loss and optimization\n",
    "        loss = nn.SmoothL1Loss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [[50, 125]]\n",
    "GAMMAS = [0.95]\n",
    "STATE_SIZE = 25\n",
    "ACTION_SIZE = 125\n",
    "\n",
    "\n",
    "\n",
    "class TrainAndPlot:\n",
    "    def __init__(self,\n",
    "                env_size = 5,\n",
    "                n_episodes=100000, \n",
    "                max_t=1000, \n",
    "                batch_sizes=[64],\n",
    "                layers = None, \n",
    "                gammas = None, \n",
    "                epsilon_min=0.05, \n",
    "                epsilon_max=1.0, \n",
    "                epsilon_decay=0.99995, \n",
    "                memory_capacity=50000, \n",
    "                device='cpu', \n",
    "                target_update_freq=1000, \n",
    "                lr=1e-4,\n",
    "                log_interval=1000,\n",
    "                mcts_iteration=1000,\n",
    "                mcts_lr=1/np.sqrt(2)):\n",
    "        \n",
    "        self.env = TactixEnvironment(board_size=env_size)\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.layers = layers if layers else LAYERS\n",
    "        self.gammas = gammas if gammas else GAMMAS\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.device = device\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.lr = lr\n",
    "        self.log_interval = log_interval\n",
    "        self.mcts_iteration = mcts_iteration\n",
    "        self.mcts_lr = mcts_lr\n",
    "\n",
    "    def run_training(self, layer_structure, gamma, models_dir, batch_size, pretrained_model_path=None):\n",
    "    \n",
    "\n",
    "        \n",
    "        # Create agent\n",
    "        agent = DQNAgent(\n",
    "            state_size=self.env.game.height ** 2,\n",
    "            action_size=self.env.game.height ** 3,\n",
    "            layer_sizes=layer_structure,\n",
    "            lr=self.lr,\n",
    "            gamma=gamma,\n",
    "            epsilon_start=self.epsilon_max,\n",
    "            epsilon_end=self.epsilon_min,\n",
    "            epsilon_decay=self.epsilon_decay,\n",
    "            memory_capacity=self.memory_capacity,\n",
    "            pretrained_model_path=pretrained_model_path\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        rewards_log = []\n",
    "        cumulative_rewards_log = []\n",
    "        win_log = []\n",
    "        epsilon_log = []\n",
    "        total_reward = 0.0  # For cumulative tracking\n",
    "        \n",
    "        # Initialize variables for tracking the ultimate best model\n",
    "        ultimate_best_win_rate = float('-inf')\n",
    "        ultimate_best_cumulative_reward = float('-inf')\n",
    "        ultimate_best_model_path = None  # Track the path of the ultimate best model\n",
    "        \n",
    "        \n",
    "        progress_bar = tqdm(range(self.n_episodes), desc=\"Initializing Training...\", unit=\"episode\", dynamic_ncols=True)\n",
    "        for episode in progress_bar:\n",
    "            state, valid_moves_mask = self.env.reset()\n",
    "\n",
    "            mcts_agent = MCTSAgent_negamax(player=-1, iterations=self.mcts_iteration, exploration_weight=self.mcts_lr)\n",
    "\n",
    "            state = state.view(-1).unsqueeze(0)         # shape: [1, state_size]\n",
    "            valid_moves_mask = valid_moves_mask.unsqueeze(0)  # shape: [1, action_size]\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                \n",
    "                if self.env.game.current_player == -1:\n",
    "                    # MCTS agent makes a move\n",
    "                    curr_node = MCTSNode(self.env.game)\n",
    "                    mcts_best_node = mcts_agent.best_child(curr_node)\n",
    "                    \n",
    "                    self.env.game = mcts_best_node.state\n",
    "                    game_ended = self.env.game.getGameEnded()\n",
    "                \n",
    "                    if game_ended and game_ended.is_ended:\n",
    "                        done = True\n",
    "                        #print('MCTS lost')\n",
    "\n",
    "                if not done:\n",
    "                    self.env.state = self.env.game.getPieces()\n",
    "                    state = self.env._get_observation()\n",
    "                    valid_moves_mask = self.env._generate_valid_moves_mask()\n",
    "                    \n",
    "                    if not (state.dim() == 2 and state.size(0) == 1 and state.size(1) == self.state_size):\n",
    "                        state = state.view(-1).unsqueeze(0)  # Shape: [1, state_size]\n",
    "                    if not (valid_moves_mask.dim() == 2 and valid_moves_mask.size(0) == 1 and valid_moves_mask.size(1) == self.action_size):\n",
    "                        valid_moves_mask = valid_moves_mask.unsqueeze(0)  # Shape: [1, action_size]\n",
    "                    \n",
    "                    action = agent.select_action(state, valid_moves_mask)\n",
    "                    next_state, reward, done = self.env.step(action) # next_state after agent made a move -> s'\n",
    "                    next_state = next_state.view(-1).unsqueeze(0)  # shape: [1, state_size]\n",
    "                    #print(f\"DQN got a reward:{reward}\")\n",
    "\n",
    "                    # Push to replay\n",
    "                    agent.memory.push((state.cpu(), \n",
    "                                    torch.tensor(action).cpu(), \n",
    "                                    reward, \n",
    "                                    next_state.cpu(),\n",
    "                                    #next_state_valid_moves_mask.cpu(), \n",
    "                                    done))\n",
    "\n",
    "                    # Train\n",
    "                    agent.train_step(batch_size)\n",
    "\n",
    "                    \n",
    "                    state = next_state\n",
    "                    \n",
    "                \n",
    "                    episode_reward += reward\n",
    "\n",
    "            \n",
    "            # Update target\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            # Logging\n",
    "            total_reward += episode_reward\n",
    "            rewards_log.append(episode_reward)\n",
    "            cumulative_rewards_log.append(total_reward)\n",
    "            win_log.append(1 if episode_reward > 0 else 0)\n",
    "            epsilon_log.append(agent.epsilon)\n",
    "\n",
    "\n",
    "            if len(win_log) >= 200:\n",
    "                avg_win_rate = 200.0 * np.mean(win_log[-200:])\n",
    "                current_cumulative_reward = cumulative_rewards_log[-1] if cumulative_rewards_log else 0\n",
    "\n",
    "                # Save the model only if:\n",
    "                # 1. The new win rate is greater than the best win rate seen so far, or\n",
    "                # 2. The new win rate equals the best win rate, but the cumulative reward is higher\n",
    "                if (avg_win_rate > ultimate_best_win_rate) or (\n",
    "                        avg_win_rate == ultimate_best_win_rate and current_cumulative_reward > ultimate_best_cumulative_reward):\n",
    "                    ultimate_best_win_rate = avg_win_rate\n",
    "                    ultimate_best_cumulative_reward = current_cumulative_reward\n",
    "\n",
    "                    # Update the model state and name\n",
    "                    ultimate_best_model_state = agent.q_network.state_dict()\n",
    "                    ultimate_best_model_name = (\n",
    "                        f\"network_hl_{'_'.join(map(str, layer_structure))}_gamma_{gamma:.2f}_\"\n",
    "                        f\"bs_{batch_size}_tufq_{self.target_update_freq}_mcts_iter_{mcts_iteration}_mcts_lr_{mcts_lr}_\"\n",
    "                        f\"wr_{int(ultimate_best_win_rate)}_tr_{int(ultimate_best_cumulative_reward)}.pth\"\n",
    "                    )\n",
    "\n",
    "                    \n",
    "            # Print progress occasionally\n",
    "            # if (episode+1) % self.log_interval == 0:  # Log interval\n",
    "            #     avg_reward = np.mean(rewards_log[-100:]) if len(rewards_log) > 100 else np.mean(rewards_log)\n",
    "            #     win_rate = 100.0 * np.mean(win_log[-100:]) if len(win_log) > 100 else 100.0 * np.mean(win_log)\n",
    "            #     print(f\"[{episode+1}/{self.n_episodes}] Layers={layer_structure}, Gamma={gamma}, \"\n",
    "            #         f\"AvgReward(Last100)={avg_reward:.2f}, WinRate(Last100)={win_rate:.2f}%, Eps={agent.epsilon:.3f}\")\n",
    "            if episode % 10000 == 0 and len(win_log) >= 200:\n",
    "                avg_reward = np.mean(rewards_log[-200:])\n",
    "                win_rate = 100.0 * np.mean(win_log[-200:])\n",
    "                progress_bar.set_description(\n",
    "                    f\"AvgReward={avg_reward:.2f}, WinRate={win_rate:.2f}%, \"\n",
    "                    f\"Eps={agent.epsilon:.3f}\"\n",
    "                )\n",
    "                \n",
    "        if ultimate_best_model_state and ultimate_best_model_name:\n",
    "            ultimate_best_model_path = os.path.join(models_dir, ultimate_best_model_name)\n",
    "            torch.save(ultimate_best_model_state, ultimate_best_model_path)\n",
    "            print(f\"Ultimate best model saved: {ultimate_best_model_path}\")\n",
    "\n",
    "        return rewards_log, cumulative_rewards_log, win_log, epsilon_log, ultimate_best_win_rate, ultimate_best_cumulative_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_experiments(self, pretrained_model_path=None):\n",
    "        \n",
    "        # Centralized directory setup\n",
    "        base_dir = \"/Users/alibal/Desktop/tactix_training\"\n",
    "        save_dir = os.path.join(base_dir,f\"training_results_{self.env.game.height}x{self.env.game.height}_randomopponent_s'_after_agent_withattention_mcts2\")\n",
    "        models_dir = os.path.join(save_dir, \"models\")\n",
    "        plots_dir = os.path.join(save_dir, \"plots\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        results = {}  # (layer_tuple, gamma) -> (rewards_log, cumulative_rewards_log, win_log, epsilon_log)\n",
    "        for batch_size in self.batch_sizes:\n",
    "            for layer in self.layers:\n",
    "                for gamma in self.gammas:\n",
    "                    print(f\"=== Training with LayerStructure={layer}, Gamma={gamma}, Batch Size={batch_size}, Epsilon(max, min)={self.epsilon_max, self.epsilon_min}, mem_cap={self.memory_capacity}, Target Update={self.target_update_freq} ===\")\n",
    "                    r_log, c_log, w_log, e_log, ultimate_best_win_rate, ultimate_best_cumulative_reward = self.run_training(layer, gamma, models_dir, batch_size, pretrained_model_path=pretrained_model_path)\n",
    "                    results[(tuple(layer), gamma)] = (r_log, c_log, w_log, e_log)\n",
    "                    \n",
    "                    # Plot results for this combination\n",
    "                    fig, axs = plt.subplots(3, 1, figsize=(16, 16))\n",
    "                    \n",
    "                    # Prepare parameter text\n",
    "                    parameter_text = (\n",
    "                        f\"n_episodes={self.n_episodes}, max_t={self.max_t}, batch_size={batch_size},\\n\"\n",
    "                        f\"board_size = {self.env.game.height}x{self.env.game.height}, layers={layer}, gamma={gamma:.2f},\\n\"\n",
    "                        f\"epsilon_min={self.epsilon_min}, epsilon_max={self.epsilon_max}, epsilon_decay={self.epsilon_decay},\\n\"\n",
    "                        f\"memory_capacity={self.memory_capacity}, device={self.device}, target_update_freq={self.target_update_freq},\\n\"\n",
    "                        f\"lr={self.lr}, mcts_iteration={self.mcts_iteration}, mcts_lr={self.mcts_lr}\"\n",
    "                    )\n",
    "                    \n",
    "                    # 1) Rewards\n",
    "                    axs[0].plot(r_log, label=\"Rewards\")\n",
    "                    rolling_avg_r = [np.mean(r_log[max(0, i-1000):i+1]) for i in range(len(r_log))]\n",
    "                    axs[0].plot(rolling_avg_r, label=\"Average Rewards (Last 1000)\")\n",
    "                    axs[0].set_xlabel(\"Episode\")\n",
    "                    axs[0].set_ylabel(\"Reward\")\n",
    "                    axs[0].set_title(f\"Rewards - Layers={layer}, Gamma={gamma}\", fontsize=14)\n",
    "                    \n",
    "                    axs[0].legend()\n",
    "                    axs[0].grid()\n",
    "                    \n",
    "                    # 2) Cumulative Rewards\n",
    "                    axs[1].plot(c_log, label=\"Cumulative Rewards\")\n",
    "                    axs[1].set_xlabel(\"Episode\")\n",
    "                    axs[1].set_ylabel(\"Total Reward\")\n",
    "                    axs[1].set_title(f\"Cumulative Rewards - Layers={layer}, Gamma={gamma}\")\n",
    "                    axs[1].legend()\n",
    "                    axs[1].grid()\n",
    "                    \n",
    "                    # 3) Win Rate\n",
    "                    rolling_win = [100.0*np.mean(w_log[max(0, i-1000):i+1]) for i in range(len(w_log))]\n",
    "                    axs[2].plot(rolling_win, label=\"Win Rate (Last 1000 Episodes)\")\n",
    "                    axs[2].set_xlabel(\"Episode\")\n",
    "                    axs[2].set_ylabel(\"Win Rate (%)\")\n",
    "                    axs[2].set_title(f\"Win Rate - Layers={layer}, Gamma={gamma}\")\n",
    "                    axs[2].legend()\n",
    "                    axs[2].grid()\n",
    "\n",
    "                \n",
    "\n",
    "                    # Add parameter text at the top\n",
    "                    fig.text(\n",
    "                        0.5, 1.02,  # Position above the subplots\n",
    "                        parameter_text,\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=9\n",
    "                    )\n",
    "\n",
    "                    # Convert the parameter_text into a single-line string for the file name\n",
    "                    parameters_for_filename = (\n",
    "                        f\"numep_{self.n_episodes}_bs_{batch_size}_\"\n",
    "                        f\"hl_{'_'.join(map(str, layer))}_\"\n",
    "                        f\"gamma_{gamma:.2f}_\"\n",
    "                        f\"mem_cap_{self.memory_capacity}_\"\n",
    "                        f\"tufq_{self.target_update_freq}_lr_{self.lr}_\"\n",
    "                        f\"wr_{int(ultimate_best_win_rate)}_tr_{int(ultimate_best_cumulative_reward)}\"\n",
    "                    )\n",
    "\n",
    "                    # Replace any characters that are invalid in file names (e.g., colons, slashes, spaces)\n",
    "                    parameters_for_filename = parameters_for_filename.replace(\":\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "                    # Adjust layout to leave space at the top for the parameter text\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.85])  # Leave 5% space at the top\n",
    "                    plt.subplots_adjust(top=0.8)  # Adjust top space explicitly\n",
    "\n",
    "                    # Save the plot\n",
    "                    plot_name = f\"{parameters_for_filename}.png\"\n",
    "                    plot_path = os.path.join(plots_dir, plot_name)\n",
    "                    plt.savefig(plot_path, bbox_inches=\"tight\")  # Save all elements, ensuring no clipping\n",
    "                    plt.show()\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training with LayerStructure=[128, 128, 128], Gamma=0.7, Batch Size=128, Epsilon(max, min)=(1.0, 0.01), mem_cap=100000, Target Update=100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Training...:   0%|          | 199/100000 [00:17<2:23:31, 11.59episode/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mcts_iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m experiment_mcts2 \u001b[38;5;241m=\u001b[39m TrainAndPlot(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m], layers\u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m]],\n\u001b[1;32m      2\u001b[0m                                                                     gammas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.7\u001b[39m], epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epsilon_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9995\u001b[39m, \n\u001b[1;32m      3\u001b[0m                                                                     memory_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, target_update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, mcts_iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, mcts_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m results_mcts \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_mcts2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[0;32mIn[82], line 210\u001b[0m, in \u001b[0;36mTrainAndPlot.run_experiments\u001b[0;34m(self, pretrained_model_path)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gamma \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgammas:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Training with LayerStructure=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Gamma=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon(max, min)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_max,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mem_cap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_capacity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Target Update=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m     r_log, c_log, w_log, e_log, ultimate_best_win_rate, ultimate_best_cumulative_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     results[(\u001b[38;5;28mtuple\u001b[39m(layer), gamma)] \u001b[38;5;241m=\u001b[39m (r_log, c_log, w_log, e_log)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# Plot results for this combination\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 163\u001b[0m, in \u001b[0;36mTrainAndPlot.run_training\u001b[0;34m(self, layer_structure, gamma, models_dir, batch_size, pretrained_model_path)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Update the model state and name\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         ultimate_best_model_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m    161\u001b[0m         ultimate_best_model_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_hl_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39mlayer_structure))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gamma_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 163\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tufq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mcts_iter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmcts_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mcts_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmcts_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(ultimate_best_win_rate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(ultimate_best_cumulative_reward)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Print progress occasionally\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# if (episode+1) % self.log_interval == 0:  # Log interval\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m#     avg_reward = np.mean(rewards_log[-100:]) if len(rewards_log) > 100 else np.mean(rewards_log)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#     win_rate = 100.0 * np.mean(win_log[-100:]) if len(win_log) > 100 else 100.0 * np.mean(win_log)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m#     print(f\"[{episode+1}/{self.n_episodes}] Layers={layer_structure}, Gamma={gamma}, \"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m#         f\"AvgReward(Last100)={avg_reward:.2f}, WinRate(Last100)={win_rate:.2f}%, Eps={agent.epsilon:.3f}\")\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(win_log) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mcts_iteration' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_mcts2 = TrainAndPlot(n_episodes=100000, max_t=1000, batch_sizes=[128], layers= [[128,128,128]],\n",
    "                                                                    gammas=[0.7], epsilon_min=0.01, epsilon_max=1.0, epsilon_decay=0.9995, \n",
    "                                                                    memory_capacity=100000, device='cpu', target_update_freq=100, lr=0.0001, log_interval=100, mcts_iteration=50, mcts_lr=0.4)\n",
    "results_mcts = experiment_mcts2.run_experiments();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactix-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
