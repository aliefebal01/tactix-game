{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(\"..\")  # Adjust based on your folder structure\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agents.mcts.mcts_agent_negamax' from '/Users/alibal/Desktop/tactix-game/agents/mcts/mcts_agent_negamax.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agents.mcts.mcts_agent_negamax import MCTSAgent_negamax\n",
    "from agents.mcts.mcts_node import MCTSNode\n",
    "from tactix.utils import *\n",
    "from tactix.tactixEnvironment_without_opp_mcts import TactixEnvironment\n",
    "from tactix.tactixGame import TactixGame\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "importlib.reload(sys.modules['tactix.tactixGame'])\n",
    "importlib.reload(sys.modules['tactix.tactixEnvironment_without_opp_mcts'])\n",
    "importlib.reload(sys.modules['tactix.utils'])\n",
    "importlib.reload(sys.modules['agents.mcts.mcts_agent_negamax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A DQN with a single attention layer after the input state.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention_layer = nn.Linear(state_size, state_size)\n",
    "        self.attention_activation = nn.Softmax(dim=-1)  # Softmax to generate attention weights\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers = []\n",
    "        input_dim = state_size\n",
    "        for hidden_size in layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "        \n",
    "        # Final layer: from the last hidden layer to the action output\n",
    "        layers.append(nn.Linear(input_dim, action_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the DQN with an attention layer.\n",
    "        x: Input state tensor, shape [batch_size, state_size].\n",
    "        \"\"\"\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention_activation(self.attention_layer(x))  # [batch_size, state_size]\n",
    "        x = x * attention_weights  # Apply attention weights to the input\n",
    "        \n",
    "        # Pass through the rest of the network\n",
    "        return self.network(x)  # Outputs Q-values for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Store a transition (s, a, r, s')\"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_size, \n",
    "        action_size, \n",
    "        layer_sizes,\n",
    "        lr=1e-3, \n",
    "        gamma=0.9, \n",
    "        epsilon_start=1.0, \n",
    "        epsilon_end=0.01, \n",
    "        epsilon_decay=0.999876,\n",
    "        memory_capacity=10000,\n",
    "        device='cpu',\n",
    "        pretrained_model_path = None\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "        \n",
    "        # Q-Networks (main + target)\n",
    "        self.q_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "\n",
    "        if pretrained_model_path:\n",
    "            self.q_network.load_state_dict(torch.load(pretrained_model_path, map_location=self.device))\n",
    "            print(f\"Loaded pretrained model from {pretrained_model_path}\")\n",
    "\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        # Replay Memory\n",
    "        self.memory = ReplayMemory(capacity=memory_capacity)\n",
    "\n",
    "    def select_action(self, state, valid_moves_mask):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection with invalid move masking.\n",
    "        state: shape (1, state_size)\n",
    "        valid_moves_mask: shape (1, action_size) -> 1/0 for valid moves\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            valid_indices = torch.where(valid_moves_mask[0] == 1)[0]\n",
    "            action = random.choice(valid_indices.tolist())\n",
    "            return action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state.to(self.device))  # (1, action_size)\n",
    "                # Mask invalid actions by setting them to -inf\n",
    "                q_values[valid_moves_mask == 0] = -float('inf')\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network to match the Q-network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    " \n",
    "    def train_step(self, batch_size):\n",
    "        \"\"\"Train the Q-network using one batch from experience replay.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Not enough samples to train\n",
    "        \n",
    "        # Sample a batch of transitions\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # transitions is a list of tuples: (state, action, reward, next_state, done)\n",
    "        batch = list(zip(*transitions))\n",
    "\n",
    "        states = torch.stack(batch[0]).to(self.device)          # shape: [batch_size, 1, state_size]\n",
    "        actions = torch.stack(batch[1]).to(self.device)         # shape: [batch_size]\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32).to(self.device)  # [batch_size]\n",
    "        next_states = torch.stack(batch[3]).to(self.device)     # shape: [batch_size, 1, state_size]\n",
    "        #next_states_valid_moves_mask = torch.stack(batch[4]).to(self.device)  # shape: [batch_size, 1, action_size]\n",
    "        dones = torch.tensor(batch[4], dtype=torch.bool).to(self.device)       # [batch_size]\n",
    "        \n",
    "        # Flatten states: we have [batch_size, 1, state_size] => [batch_size, state_size]\n",
    "        states = states.view(states.size(0), -1)\n",
    "        next_states = next_states.view(next_states.size(0), -1)\n",
    "\n",
    "        # Flatten next_states_valid_moves_mask: [batch_size, 1, action_size] => [batch_size, action_size]\n",
    "        #next_states_valid_moves_mask = next_states_valid_moves_mask.view(next_states_valid_moves_mask.size(0), -1)\n",
    "\n",
    "        # Current Q-values\n",
    "        q_values = self.q_network(states)\n",
    "        # Gather Q-values for the taken actions\n",
    "        # q_values shape is [batch_size, action_size], actions is [batch_size]\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():  \n",
    "            #temp_next_q_values = self.target_network(next_states)\n",
    "            #temp_next_q_values[next_states_valid_moves_mask == 0] = -float('inf')\n",
    "            #max_next_q_values = temp_next_q_values.max(1)[0]\n",
    "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones.float()) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # Loss and optimization\n",
    "        loss = nn.SmoothL1Loss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [[50, 125]]\n",
    "GAMMAS = [0.95]\n",
    "STATE_SIZE = 25\n",
    "ACTION_SIZE = 125\n",
    "\n",
    "\n",
    "\n",
    "class TrainAndPlot:\n",
    "    def __init__(self,\n",
    "                env_size = 5,\n",
    "                n_episodes=100000, \n",
    "                max_t=1000, \n",
    "                batch_sizes=[64],\n",
    "                layers = None, \n",
    "                gammas = None, \n",
    "                epsilon_min=0.05, \n",
    "                epsilon_max=1.0, \n",
    "                epsilon_decay=0.99995, \n",
    "                memory_capacity=50000, \n",
    "                device='cpu', \n",
    "                target_update_freq=1000, \n",
    "                lr=1e-4,\n",
    "                log_interval=1000,\n",
    "                mcts_iteration=1000,\n",
    "                mcts_lr=1/np.sqrt(2)):\n",
    "        \n",
    "        self.env = TactixEnvironment(board_size=env_size)\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.layers = layers if layers else LAYERS\n",
    "        self.gammas = gammas if gammas else GAMMAS\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.device = device\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.lr = lr\n",
    "        self.log_interval = log_interval\n",
    "        self.mcts_iteration = mcts_iteration\n",
    "        self.mcts_lr = mcts_lr\n",
    "\n",
    "    def run_training(self, layer_structure, gamma, models_dir, batch_size, pretrained_model_path=None):\n",
    "    \n",
    "\n",
    "        \n",
    "        # Create agent\n",
    "        agent = DQNAgent(\n",
    "            state_size=self.env.board_size ** 2,\n",
    "            action_size=self.env.board_size ** 3,\n",
    "            layer_sizes=layer_structure,\n",
    "            lr=self.lr,\n",
    "            gamma=gamma,\n",
    "            epsilon_start=self.epsilon_max,\n",
    "            epsilon_end=self.epsilon_min,\n",
    "            epsilon_decay=self.epsilon_decay,\n",
    "            memory_capacity=self.memory_capacity,\n",
    "            pretrained_model_path=pretrained_model_path\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        rewards_log = []\n",
    "        cumulative_rewards_log = []\n",
    "        win_log = []\n",
    "        epsilon_log = []\n",
    "        total_reward = 0.0  # For cumulative tracking\n",
    "        \n",
    "        # Initialize variables for tracking the ultimate best model\n",
    "        ultimate_best_win_rate = float('-inf')\n",
    "        ultimate_best_cumulative_reward = float('-inf')\n",
    "        ultimate_best_model_path = None  # Track the path of the ultimate best model\n",
    "        \n",
    "        \n",
    "        progress_bar = tqdm(range(self.n_episodes), desc=\"Initializing Training...\", unit=\"episode\", dynamic_ncols=True)\n",
    "        for episode in progress_bar:\n",
    "            state, valid_moves_mask = self.env.reset()\n",
    "\n",
    "            mcts_agent = MCTSAgent_negamax(player=-1, iterations=self.mcts_iteration, exploration_weight=self.mcts_lr)\n",
    "\n",
    "            state = state.view(-1).unsqueeze(0)         # shape: [1, state_size]\n",
    "            valid_moves_mask = valid_moves_mask.unsqueeze(0)  # shape: [1, action_size]\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                \n",
    "                if self.env.game.current_player == -1:\n",
    "                    # MCTS agent makes a move\n",
    "                    curr_node = MCTSNode(self.env.game)\n",
    "                    mcts_best_node = mcts_agent.best_child(curr_node)\n",
    "                    \n",
    "                    self.env.game = mcts_best_node.state\n",
    "                    game_ended = self.env.game.getGameEnded()\n",
    "                \n",
    "                    if game_ended and game_ended.is_ended:\n",
    "                        done = True\n",
    "                        #print('MCTS lost')\n",
    "\n",
    "                if not done:\n",
    "                    self.env.state = self.env.game.getPieces()\n",
    "                    state = self.env._get_observation()\n",
    "                    valid_moves_mask = self.env._generate_valid_moves_mask()\n",
    "                    \n",
    "                    if not (state.dim() == 2 and state.size(0) == 1 and state.size(1) == self.state_size):\n",
    "                        state = state.view(-1).unsqueeze(0)  # Shape: [1, state_size]\n",
    "                    if not (valid_moves_mask.dim() == 2 and valid_moves_mask.size(0) == 1 and valid_moves_mask.size(1) == self.action_size):\n",
    "                        valid_moves_mask = valid_moves_mask.unsqueeze(0)  # Shape: [1, action_size]\n",
    "                    \n",
    "                    action = agent.select_action(state, valid_moves_mask)\n",
    "                    next_state, reward, done = self.env.step(action) # next_state after agent made a move -> s'\n",
    "                    next_state = next_state.view(-1).unsqueeze(0)  # shape: [1, state_size]\n",
    "                    #print(f\"DQN got a reward:{reward}\")\n",
    "\n",
    "                    # Push to replay\n",
    "                    agent.memory.push((state.cpu(), \n",
    "                                    torch.tensor(action).cpu(), \n",
    "                                    reward, \n",
    "                                    next_state.cpu(),\n",
    "                                    #next_state_valid_moves_mask.cpu(), \n",
    "                                    done))\n",
    "\n",
    "                    # Train\n",
    "                    agent.train_step(batch_size)\n",
    "\n",
    "                    \n",
    "                    state = next_state\n",
    "                    \n",
    "                \n",
    "                    episode_reward += reward\n",
    "\n",
    "            \n",
    "            # Update target\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            # Logging\n",
    "            total_reward += episode_reward\n",
    "            rewards_log.append(episode_reward)\n",
    "            cumulative_rewards_log.append(total_reward)\n",
    "            win_log.append(1 if episode_reward > 0 else 0)\n",
    "            epsilon_log.append(agent.epsilon)\n",
    "\n",
    "\n",
    "            if len(win_log) >= 200:\n",
    "                avg_win_rate = 200.0 * np.mean(win_log[-200:])\n",
    "                current_cumulative_reward = cumulative_rewards_log[-1] if cumulative_rewards_log else 0\n",
    "\n",
    "                # Save the model only if:\n",
    "                # 1. The new win rate is greater than the best win rate seen so far, or\n",
    "                # 2. The new win rate equals the best win rate, but the cumulative reward is higher\n",
    "                if (avg_win_rate > ultimate_best_win_rate) or (\n",
    "                        avg_win_rate == ultimate_best_win_rate and current_cumulative_reward > ultimate_best_cumulative_reward):\n",
    "                    ultimate_best_win_rate = avg_win_rate\n",
    "                    ultimate_best_cumulative_reward = current_cumulative_reward\n",
    "\n",
    "                    # Update the model state and name\n",
    "                    ultimate_best_model_state = agent.q_network.state_dict()\n",
    "                    ultimate_best_model_name = (\n",
    "                        f\"network_hl_{'_'.join(map(str, layer_structure))}_gamma_{gamma:.2f}_\"\n",
    "                        f\"bs_{batch_size}_tufq_{self.target_update_freq}_mcts_iter_{mcts_iteration}_mcts_lr_{mcts_lr}_\"\n",
    "                        f\"wr_{int(ultimate_best_win_rate)}_tr_{int(ultimate_best_cumulative_reward)}.pth\"\n",
    "                    )\n",
    "\n",
    "                    \n",
    "            # Print progress occasionally\n",
    "            # if (episode+1) % self.log_interval == 0:  # Log interval\n",
    "            #     avg_reward = np.mean(rewards_log[-100:]) if len(rewards_log) > 100 else np.mean(rewards_log)\n",
    "            #     win_rate = 100.0 * np.mean(win_log[-100:]) if len(win_log) > 100 else 100.0 * np.mean(win_log)\n",
    "            #     print(f\"[{episode+1}/{self.n_episodes}] Layers={layer_structure}, Gamma={gamma}, \"\n",
    "            #         f\"AvgReward(Last100)={avg_reward:.2f}, WinRate(Last100)={win_rate:.2f}%, Eps={agent.epsilon:.3f}\")\n",
    "            if episode % 10000 == 0 and len(win_log) >= 200:\n",
    "                avg_reward = np.mean(rewards_log[-200:])\n",
    "                win_rate = 100.0 * np.mean(win_log[-200:])\n",
    "                progress_bar.set_description(\n",
    "                    f\"AvgReward={avg_reward:.2f}, WinRate={win_rate:.2f}%, \"\n",
    "                    f\"Eps={agent.epsilon:.3f}\"\n",
    "                )\n",
    "                \n",
    "        if ultimate_best_model_state and ultimate_best_model_name:\n",
    "            ultimate_best_model_path = os.path.join(models_dir, ultimate_best_model_name)\n",
    "            torch.save(ultimate_best_model_state, ultimate_best_model_path)\n",
    "            print(f\"Ultimate best model saved: {ultimate_best_model_path}\")\n",
    "\n",
    "        return rewards_log, cumulative_rewards_log, win_log, epsilon_log, ultimate_best_win_rate, ultimate_best_cumulative_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_experiments(self, pretrained_model_path=None):\n",
    "        \n",
    "        # Centralized directory setup\n",
    "        base_dir = \"/Users/alibal/Desktop/tactix_training\"\n",
    "        save_dir = os.path.join(base_dir,f\"training_results_{self.env.game.height}x{self.env.game.height}_randomopponent_s'_after_agent_withattention_mcts2\")\n",
    "        models_dir = os.path.join(save_dir, \"models\")\n",
    "        plots_dir = os.path.join(save_dir, \"plots\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        results = {}  # (layer_tuple, gamma) -> (rewards_log, cumulative_rewards_log, win_log, epsilon_log)\n",
    "        for batch_size in self.batch_sizes:\n",
    "            for layer in self.layers:\n",
    "                for gamma in self.gammas:\n",
    "                    print(f\"=== Training with LayerStructure={layer}, Gamma={gamma}, Batch Size={batch_size}, Epsilon(max, min)={self.epsilon_max, self.epsilon_min}, mem_cap={self.memory_capacity}, Target Update={self.target_update_freq} ===\")\n",
    "                    r_log, c_log, w_log, e_log, ultimate_best_win_rate, ultimate_best_cumulative_reward = self.run_training(layer, gamma, models_dir, batch_size, pretrained_model_path=pretrained_model_path)\n",
    "                    results[(tuple(layer), gamma)] = (r_log, c_log, w_log, e_log)\n",
    "                    \n",
    "                    # Plot results for this combination\n",
    "                    fig, axs = plt.subplots(3, 1, figsize=(16, 16))\n",
    "                    \n",
    "                    # Prepare parameter text\n",
    "                    parameter_text = (\n",
    "                        f\"n_episodes={self.n_episodes}, max_t={self.max_t}, batch_size={batch_size},\\n\"\n",
    "                        f\"board_size = {self.env.game.height}x{self.env.game.height}, layers={layer}, gamma={gamma:.2f},\\n\"\n",
    "                        f\"epsilon_min={self.epsilon_min}, epsilon_max={self.epsilon_max}, epsilon_decay={self.epsilon_decay},\\n\"\n",
    "                        f\"memory_capacity={self.memory_capacity}, device={self.device}, target_update_freq={self.target_update_freq},\\n\"\n",
    "                        f\"lr={self.lr}, mcts_iteration={self.mcts_iteration}, mcts_lr={self.mcts_lr}\"\n",
    "                    )\n",
    "                    \n",
    "                    # 1) Rewards\n",
    "                    axs[0].plot(r_log, label=\"Rewards\")\n",
    "                    rolling_avg_r = [np.mean(r_log[max(0, i-1000):i+1]) for i in range(len(r_log))]\n",
    "                    axs[0].plot(rolling_avg_r, label=\"Average Rewards (Last 1000)\")\n",
    "                    axs[0].set_xlabel(\"Episode\")\n",
    "                    axs[0].set_ylabel(\"Reward\")\n",
    "                    axs[0].set_title(f\"Rewards - Layers={layer}, Gamma={gamma}\", fontsize=14)\n",
    "                    \n",
    "                    axs[0].legend()\n",
    "                    axs[0].grid()\n",
    "                    \n",
    "                    # 2) Cumulative Rewards\n",
    "                    axs[1].plot(c_log, label=\"Cumulative Rewards\")\n",
    "                    axs[1].set_xlabel(\"Episode\")\n",
    "                    axs[1].set_ylabel(\"Total Reward\")\n",
    "                    axs[1].set_title(f\"Cumulative Rewards - Layers={layer}, Gamma={gamma}\")\n",
    "                    axs[1].legend()\n",
    "                    axs[1].grid()\n",
    "                    \n",
    "                    # 3) Win Rate\n",
    "                    rolling_win = [100.0*np.mean(w_log[max(0, i-1000):i+1]) for i in range(len(w_log))]\n",
    "                    axs[2].plot(rolling_win, label=\"Win Rate (Last 1000 Episodes)\")\n",
    "                    axs[2].set_xlabel(\"Episode\")\n",
    "                    axs[2].set_ylabel(\"Win Rate (%)\")\n",
    "                    axs[2].set_title(f\"Win Rate - Layers={layer}, Gamma={gamma}\")\n",
    "                    axs[2].legend()\n",
    "                    axs[2].grid()\n",
    "\n",
    "                \n",
    "\n",
    "                    # Add parameter text at the top\n",
    "                    fig.text(\n",
    "                        0.5, 1.02,  # Position above the subplots\n",
    "                        parameter_text,\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=9\n",
    "                    )\n",
    "\n",
    "                    # Convert the parameter_text into a single-line string for the file name\n",
    "                    parameters_for_filename = (\n",
    "                        f\"numep_{self.n_episodes}_bs_{batch_size}_\"\n",
    "                        f\"hl_{'_'.join(map(str, layer))}_\"\n",
    "                        f\"gamma_{gamma:.2f}_\"\n",
    "                        f\"mem_cap_{self.memory_capacity}_\"\n",
    "                        f\"tufq_{self.target_update_freq}_lr_{self.lr}_\"\n",
    "                        f\"wr_{int(ultimate_best_win_rate)}_tr_{int(ultimate_best_cumulative_reward)}\"\n",
    "                    )\n",
    "\n",
    "                    # Replace any characters that are invalid in file names (e.g., colons, slashes, spaces)\n",
    "                    parameters_for_filename = parameters_for_filename.replace(\":\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "                    # Adjust layout to leave space at the top for the parameter text\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.85])  # Leave 5% space at the top\n",
    "                    plt.subplots_adjust(top=0.8)  # Adjust top space explicitly\n",
    "\n",
    "                    # Save the plot\n",
    "                    plot_name = f\"{parameters_for_filename}.png\"\n",
    "                    plot_path = os.path.join(plots_dir, plot_name)\n",
    "                    plt.savefig(plot_path, bbox_inches=\"tight\")  # Save all elements, ensuring no clipping\n",
    "                    plt.show()\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactix-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
