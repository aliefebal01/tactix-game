{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(\"..\")  # Adjust based on your folder structure\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tactix.utils' from '/Users/alibal/Desktop/tactix-game/tactix/utils.py'>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from tactix.utils import *\n",
    "from tactix.tactixEnvironment_without_opp import TactixEnvironment\n",
    "from tactix.tactixGame import TactixGame\n",
    "from scripts_1.dqn_agent_masked import DQNAgent as DQNAgentMasked\n",
    "from scripts.dqn_agent_attention import DQNAgent\n",
    "from scripts.dqn_agent_mh_attention import DQNAgent as DQNAgentMH\n",
    "from scripts.dqn_agent_dr_re_attention import DQNAgent as DQNAgentDR\n",
    "from scripts.train_without_opp_move_attention_mcts2 import TrainAndPlot_without_opp_move_attention_mcts2\n",
    "from scripts.dqn_attention import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importlib.reload(sys.modules['tactix.tactixGame'])\n",
    "importlib.reload(sys.modules['tactix.tactixEnvironment_without_opp'])\n",
    "importlib.reload(sys.modules['tactix.utils'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_3x3_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_10_mcts_lr_0.4_wr_76_tr_-22962.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_3x3_small = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_3x3_with_attention_mcts2\",\n",
    "        \"models\",\n",
    "        \"network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_10_mcts_lr_0.4_wr_76_tr_-22962.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_3x3_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_29_tr_-179250.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_small = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_5x5_with_attention_mcts2\",\n",
    "        \"models\",\n",
    "        \"network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_29_tr_-179250.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_29_tr_-179250.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_small = DQNAgent(state_size=25, action_size=125, layer_sizes=[128, 128, 128],\n",
    "                           lr=1e-4, gamma=0.7, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=50000, device=\"cpu\", pretrained_model_path=evaluated_model_path_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_256_256_256_256_256_gamma_0.70_bs_256_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_54_tr_-178538.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_medium = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_5x5_with_attention_mcts2\",\n",
    "        \"models\",\n",
    "        \"network_hl_256_256_256_256_256_gamma_0.70_bs_256_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_54_tr_-178538.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_36_tr_-279934.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_end_game = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_5x5_with_attention_mcts2\",\n",
    "        \"models\",\n",
    "        \"network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_36_tr_-279934.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_end_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_50_mcts_lr_0.4_wr_36_tr_-279934.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_end_game = DQNAgent(state_size=25, action_size=125, layer_sizes=[128, 128, 128],\n",
    "                           lr=1e-4, gamma=0.75, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=50000, device=\"cpu\", pretrained_model_path=evaluated_model_path_end_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_dqns_against_each_other/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_100_wr_96_tr_114516.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_dqn_trained = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_5x5_dqns_against_each_other\",\n",
    "        \"models\",\n",
    "        \"network_hl_128_128_128_gamma_0.70_bs_128_tufq_100_wr_96_tr_114516.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_dqn_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_dqns_against_each_other/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_100_wr_96_tr_114516.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_dqn = DQNAgent(state_size=25, action_size=125, layer_sizes=[128, 128, 128],\n",
    "                           lr=1e-4, gamma=0.7, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=50000, device=\"cpu\", pretrained_model_path=evaluated_model_path_dqn_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_256_256_256_256_256_gamma_0.70_bs_256_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_54_tr_-178538.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_medium = DQNAgent(state_size=25, action_size=125, layer_sizes=[256, 256, 256, 256, 256],\n",
    "                           lr=1e-4, gamma=0.7, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=50000, device=\"cpu\", pretrained_model_path=evaluated_model_path_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_512_512_512_512_512_512_512_gamma_0.70_bs_512_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_47_tr_-104389.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_model_path_big = os.path.join(\n",
    "        project_root,\n",
    "        \"training_results\",\n",
    "        \"training_results_5x5_with_attention_mcts2\",\n",
    "        \"models\",\n",
    "        \"network_hl_512_512_512_512_512_512_512_gamma_0.70_bs_512_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_47_tr_-104389.pth\"\n",
    "    )\n",
    "print(evaluated_model_path_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_5x5_with_attention_mcts2/models/network_hl_512_512_512_512_512_512_512_gamma_0.70_bs_512_tufq_200_mcts_iter_5_mcts_lr_0.4_wr_47_tr_-104389.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibal/Desktop/tactix-game/scripts/dqn_agent_mh_attention.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.q_network.load_state_dict(torch.load(pretrained_model_path, map_location=self.device))\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_big = DQNAgentMH(state_size=25, action_size=125, layer_sizes=[512, 512, 512, 512, 512, 512, 512],\n",
    "                           lr=1e-4, gamma=0.7, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=50000, device=\"cpu\", pretrained_model_path=evaluated_model_path_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from /Users/alibal/Desktop/tactix-game/training_results/training_results_3x3_with_attention_mcts2/models/network_hl_128_128_128_gamma_0.70_bs_128_tufq_200_mcts_iter_10_mcts_lr_0.4_wr_76_tr_-22962.pth\n"
     ]
    }
   ],
   "source": [
    "evaluated_agent_small_3x3 = DQNAgent(state_size=9, action_size=27, layer_sizes=[128, 128, 128],\n",
    "                           lr=1e-4, gamma=0.7, epsilon_start=0.0, epsilon_end=0.0, epsilon_decay=0.9995,\n",
    "                           memory_capacity=25000, device=\"cpu\", pretrained_model_path=evaluated_model_path_3x3_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "game3 = TactixGame(height=3, width=3, np_pieces=np.array([[1,1,0],\n",
    "                                                          [1,1,0],\n",
    "                                                          [1,0,0],]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces = game3.getPieces()\n",
    "pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces = torch.from_numpy(np.array(pieces, dtype=np.float32))\n",
    "pieces = pieces.view(-1).unsqueeze(0)\n",
    "pieces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces = pieces.reshape(3,3)\n",
    "pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "env5 = TactixEnvironment(board_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env5.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----------------------\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n",
      " -----------------------\n",
      "Current player: -1\n",
      "Game continues, no winner yet.\n"
     ]
    }
   ],
   "source": [
    "env5.game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----------------------\n",
      "[[1 0 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n",
      " -----------------------\n",
      "Current player: 1\n",
      "Game continues, no winner yet.\n"
     ]
    }
   ],
   "source": [
    "env5.step(5)\n",
    "env5.game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validler = env5._generate_valid_moves_mask()\n",
    "validler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_masked = DQNAgentMasked(state_size=25, action_size=125, layer_sizes=[128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "state = env5._get_observation()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():  \n",
    "#             temp_next_q_values = self.target_network(next_states)\n",
    "#             assert temp_next_q_values.shape == next_states_valid_moves_mask.shape\n",
    "#             temp_next_q_values[next_states_valid_moves_mask == 0] = float('inf')\n",
    "#             max_next_q_values = temp_next_q_values.min(1)[0]\n",
    "#             # max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "#             target_q_values = rewards + (1 - dones.float()) * self.gamma * max_next_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = state.view(-1).unsqueeze(0)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "validler = validler.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2560e-02,  4.4743e-02, -8.3335e-02, -1.5040e-02, -1.8834e-01,\n",
       "         -7.6102e-02, -7.4764e-02,  2.8904e-02, -1.2136e-01,  5.9650e-02,\n",
       "         -1.9152e-04,  2.7374e-02,  1.7062e-02, -4.4029e-02, -1.9325e-03,\n",
       "         -1.0993e-01, -8.8226e-02,  9.2174e-02,  4.4580e-02,  5.0101e-02,\n",
       "         -1.7164e-04,  7.5247e-02, -1.1153e-01, -5.8896e-02, -2.8726e-02,\n",
       "          4.0087e-02, -7.5686e-02, -2.7176e-02,  1.9063e-02, -2.3679e-02,\n",
       "         -1.4745e-02,  3.7237e-02,  3.0449e-02, -1.4795e-01,  1.1664e-02,\n",
       "         -6.3705e-02,  2.6341e-02, -5.9780e-02,  5.7212e-02,  8.4586e-02,\n",
       "         -2.3652e-02, -8.0456e-02, -1.2380e-01, -5.0906e-02,  1.1485e-01,\n",
       "          7.8896e-02, -6.0614e-02, -7.4042e-02, -7.7887e-02,  7.0329e-02,\n",
       "         -1.2664e-02,  2.8458e-02, -2.2329e-02, -6.0761e-02, -1.1021e-02,\n",
       "         -1.6348e-01, -8.3803e-02,  2.4966e-03,  7.9257e-02,  3.3957e-03,\n",
       "          3.6469e-02,  2.0491e-02,  1.5299e-02,  9.9019e-02, -7.4731e-02,\n",
       "         -4.5315e-02, -7.4262e-02, -4.8410e-02,  6.8960e-02, -1.2549e-02,\n",
       "          2.5297e-02, -2.6988e-02,  6.4990e-02,  9.4909e-03,  3.2584e-02,\n",
       "         -3.7064e-02, -6.9673e-02, -1.3140e-01,  1.2219e-01, -6.9837e-02,\n",
       "         -4.6973e-02, -3.2639e-02, -4.9965e-02, -4.5554e-02,  1.6086e-02,\n",
       "         -2.7934e-02,  7.1073e-02, -7.3942e-03,  3.4804e-02, -5.0865e-02,\n",
       "          4.5579e-02,  1.0259e-01,  8.5712e-02, -2.0009e-02, -1.6304e-02,\n",
       "          1.1026e-02, -9.1429e-02, -5.0623e-02,  1.9196e-02, -1.7824e-01,\n",
       "         -1.1966e-01, -5.0966e-02, -5.0242e-02, -1.1498e-01,  1.2363e-01,\n",
       "         -2.0836e-02, -1.1282e-01,  7.9041e-02,  1.2581e-02, -1.3878e-01,\n",
       "          1.0061e-01, -4.2182e-02, -6.4233e-02, -2.9850e-02, -1.0301e-02,\n",
       "         -4.3313e-02, -1.0421e-01,  1.1512e-02,  1.1891e-02,  5.8272e-02,\n",
       "          5.8402e-02,  3.8704e-02,  1.1766e-01, -7.9300e-03, -7.6647e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " qs = dqn_masked.target_network(state)\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2560e-02,         inf,         inf,         inf,         inf,\n",
       "                 inf,         inf,         inf,         inf,  5.9650e-02,\n",
       "         -1.9152e-04,  2.7374e-02,  1.7062e-02, -4.4029e-02, -1.9325e-03,\n",
       "         -1.0993e-01, -8.8226e-02,  9.2174e-02,  4.4580e-02,  5.0101e-02,\n",
       "         -1.7164e-04,  7.5247e-02, -1.1153e-01, -5.8896e-02, -2.8726e-02,\n",
       "          4.0087e-02, -7.5686e-02, -2.7176e-02,  1.9063e-02, -2.3679e-02,\n",
       "         -1.4745e-02,  3.7237e-02,  3.0449e-02, -1.4795e-01,  1.1664e-02,\n",
       "         -6.3705e-02,  2.6341e-02, -5.9780e-02,  5.7212e-02,  8.4586e-02,\n",
       "         -2.3652e-02, -8.0456e-02, -1.2380e-01, -5.0906e-02,  1.1485e-01,\n",
       "          7.8896e-02, -6.0614e-02, -7.4042e-02, -7.7887e-02,  7.0329e-02,\n",
       "         -1.2664e-02,  2.8458e-02, -2.2329e-02, -6.0761e-02, -1.1021e-02,\n",
       "         -1.6348e-01, -8.3803e-02,  2.4966e-03,  7.9257e-02,  3.3957e-03,\n",
       "          3.6469e-02,  2.0491e-02,  1.5299e-02,  9.9019e-02, -7.4731e-02,\n",
       "         -4.5315e-02, -7.4262e-02, -4.8410e-02,  6.8960e-02, -1.2549e-02,\n",
       "          2.5297e-02, -2.6988e-02,  6.4990e-02,  9.4909e-03,  3.2584e-02,\n",
       "         -3.7064e-02, -6.9673e-02, -1.3140e-01,  1.2219e-01, -6.9837e-02,\n",
       "         -4.6973e-02, -3.2639e-02, -4.9965e-02, -4.5554e-02,  1.6086e-02,\n",
       "                 inf,         inf,         inf,         inf, -5.0865e-02,\n",
       "          4.5579e-02,  1.0259e-01,  8.5712e-02, -2.0009e-02, -1.6304e-02,\n",
       "          1.1026e-02, -9.1429e-02, -5.0623e-02,  1.9196e-02, -1.7824e-01,\n",
       "         -1.1966e-01, -5.0966e-02, -5.0242e-02, -1.1498e-01,  1.2363e-01,\n",
       "         -2.0836e-02, -1.1282e-01,  7.9041e-02,  1.2581e-02, -1.3878e-01,\n",
       "          1.0061e-01, -4.2182e-02, -6.4233e-02, -2.9850e-02, -1.0301e-02,\n",
       "         -4.3313e-02, -1.0421e-01,  1.1512e-02,  1.1891e-02,  5.8272e-02,\n",
       "          5.8402e-02,  3.8704e-02,  1.1766e-01, -7.9300e-03, -7.6647e-02]],\n",
       "       grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[validler == 0] = float('inf')\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1782], grad_fn=<MinBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = qs.min(1)[0]\n",
    "q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env3 = TactixEnvironment(board_size=3)\n",
    "env3.game = game3\n",
    "env3.state = env3.game.getPieces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_gets_state = env3._get_observation()\n",
    "dqn_gets_valid_moves = env3._generate_valid_moves_mask()\n",
    "\n",
    "dqn_gets_state = dqn_gets_state.view(-1).unsqueeze(0)       # flatten the state and add a batch dimension\n",
    "dqn_gets_valid_moves = dqn_gets_valid_moves.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[0, 1, 2, True]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_small_3x3.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 3)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TactixGame(height=5, width=5, np_pieces=np.array([[0,0,0,0,0],\n",
    "                                                         [0,1,0,0,0],\n",
    "                                                         [0,0,0,0,0],\n",
    "                                                         [1,0,0,0,0],\n",
    "                                                         [1,0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TactixEnvironment(board_size=5)\n",
    "env.game = game\n",
    "env.state = env.game.getPieces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_gets_state = env._get_observation()\n",
    "dqn_gets_valid_moves = env._generate_valid_moves_mask()\n",
    "\n",
    "dqn_gets_state = dqn_gets_state.view(-1).unsqueeze(0)       # flatten the state and add a batch dimension\n",
    "dqn_gets_valid_moves = dqn_gets_valid_moves.unsqueeze(0)    # add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[3, 0, 1, False]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_small.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 5)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "[1, 0, 2, True]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_big.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 5)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "[2, 2, 2, False]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_end_game.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 5)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[1, 3, 1, False]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_medium.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 5)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "[2, 4, 1, False]\n"
     ]
    }
   ],
   "source": [
    "action_idx = evaluated_agent_dqn.select_action(dqn_gets_state, dqn_gets_valid_moves)\n",
    "print(action_idx)\n",
    "decoded_action = decode_action(action_idx, 5)\n",
    "print(decoded_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactix-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
