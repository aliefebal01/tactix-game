{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(\"..\")  # Adjust based on your folder structure\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tactix.utils' from '/Users/alibal/Desktop/tactix-game/tactix/utils.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tactix.utils import *\n",
    "from tactix.tactixEnvironment_with_opp_random import TactixEnvironment\n",
    "from tactix.tactixGame import TactixGame\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "importlib.reload(sys.modules['tactix.tactixGame'])\n",
    "importlib.reload(sys.modules['tactix.tactixEnvironment'])\n",
    "importlib.reload(sys.modules['tactix.utils'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A DQN that takes a list of hidden layer sizes as parameter.\n",
    "    Example layer_sizes = [50, 125] -> 2 hidden layers of sizes 50 and 125.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Create a list of linear layers\n",
    "        # E.g., if layer_sizes=[50,125], \n",
    "        # we want Linear(state_size, 50), then Linear(50,125), then Linear(125,action_size)\n",
    "        \n",
    "        # Build a sequence of Linear + ReLU (except for the output layer)\n",
    "        layers = []\n",
    "        input_dim = state_size\n",
    "        \n",
    "        for hidden_size in layer_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "        \n",
    "        # Final layer: from the last hidden to the action output\n",
    "        layers.append(nn.Linear(input_dim, action_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Outputs Q-values for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Store a transition (s, a, r, s')\"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_size, \n",
    "        action_size, \n",
    "        layer_sizes,\n",
    "        lr=1e-3, \n",
    "        gamma=0.9, \n",
    "        epsilon_start=1.0, \n",
    "        epsilon_end=0.01, \n",
    "        epsilon_decay=0.999876,\n",
    "        memory_capacity=10000,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "        \n",
    "        # Q-Networks (main + target)\n",
    "        self.q_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, layer_sizes).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        # Replay Memory\n",
    "        self.memory = ReplayMemory(capacity=memory_capacity)\n",
    "\n",
    "    def select_action(self, state, valid_moves_mask):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection with invalid move masking.\n",
    "        state: shape (1, state_size)\n",
    "        valid_moves_mask: shape (1, action_size) -> 1/0 for valid moves\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            valid_indices = torch.where(valid_moves_mask[0] == 1)[0]\n",
    "            action = random.choice(valid_indices.tolist())\n",
    "            return action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state.to(self.device))  # (1, action_size)\n",
    "                # Mask invalid actions by setting them to -inf\n",
    "                q_values[valid_moves_mask == 0] = -float('inf')\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network to match the Q-network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    " \n",
    "    def train_step(self, batch_size):\n",
    "        \"\"\"Train the Q-network using one batch from experience replay.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Not enough samples to train\n",
    "        \n",
    "        # Sample a batch of transitions\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # transitions is a list of tuples: (state, action, reward, next_state, done)\n",
    "        batch = list(zip(*transitions))\n",
    "\n",
    "        states = torch.stack(batch[0]).to(self.device)          # shape: [batch_size, 1, state_size]\n",
    "        actions = torch.stack(batch[1]).to(self.device)         # shape: [batch_size]\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32).to(self.device)  # [batch_size]\n",
    "        next_states = torch.stack(batch[3]).to(self.device)     # shape: [batch_size, 1, state_size]\n",
    "        #next_states_valid_moves_mask = torch.stack(batch[4]).to(self.device)  # shape: [batch_size, 1, action_size]\n",
    "        dones = torch.tensor(batch[4], dtype=torch.bool).to(self.device)       # [batch_size]\n",
    "        \n",
    "        # Flatten states: we have [batch_size, 1, state_size] => [batch_size, state_size]\n",
    "        states = states.view(states.size(0), -1)\n",
    "        next_states = next_states.view(next_states.size(0), -1)\n",
    "\n",
    "        # Flatten next_states_valid_moves_mask: [batch_size, 1, action_size] => [batch_size, action_size]\n",
    "        #next_states_valid_moves_mask = next_states_valid_moves_mask.view(next_states_valid_moves_mask.size(0), -1)\n",
    "\n",
    "        # Current Q-values\n",
    "        q_values = self.q_network(states)\n",
    "        # Gather Q-values for the taken actions\n",
    "        # q_values shape is [batch_size, action_size], actions is [batch_size]\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():  \n",
    "            #temp_next_q_values = self.target_network(next_states)\n",
    "            #temp_next_q_values[next_states_valid_moves_mask == 0] = -float('inf')\n",
    "            #max_next_q_values = temp_next_q_values.max(1)[0]\n",
    "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones.float()) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # Loss and optimization\n",
    "        loss = nn.SmoothL1Loss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [[50, 125], [125, 250, 250, 125]]\n",
    "GAMMAS = [0.95, 0.9, 0.85]\n",
    "STATE_SIZE = 25\n",
    "ACTION_SIZE = 125\n",
    "\n",
    "\n",
    "\n",
    "class TrainAndPlot:\n",
    "    def __init__(self,\n",
    "                env = None,\n",
    "                n_episodes=100000, \n",
    "                max_t=1000, \n",
    "                batch_size=64, \n",
    "                state_size = None, \n",
    "                action_size = None, \n",
    "                layers = None, \n",
    "                gammas = None, \n",
    "                epsilon_min=0.05, \n",
    "                epsilon_max=1.0, \n",
    "                epsilon_decay=0.99995, \n",
    "                memory_capacity=50000, \n",
    "                device='cpu', \n",
    "                target_update_freq=1000, \n",
    "                lr=1e-4,\n",
    "                log_interval=1000):\n",
    "        \n",
    "        self.env = env if env else TactixEnvironment()\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size if state_size else STATE_SIZE\n",
    "        self.action_size = action_size if action_size else ACTION_SIZE\n",
    "        self.layers = layers if layers else LAYERS\n",
    "        self.gammas = gammas if gammas else GAMMAS\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.device = device\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.lr = lr\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "    def run_training(self, layer_structure, gamma, models_dir):\n",
    "    \n",
    "\n",
    "\n",
    "        # Create agent\n",
    "        agent = DQNAgent(\n",
    "            state_size=self.state_size,\n",
    "            action_size=self.action_size,\n",
    "            layer_sizes=layer_structure,\n",
    "            lr=self.lr,\n",
    "            gamma=gamma,\n",
    "            epsilon_start=self.epsilon_max,\n",
    "            epsilon_end=self.epsilon_min,\n",
    "            epsilon_decay=self.epsilon_decay,\n",
    "            memory_capacity=self.memory_capacity\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        rewards_log = []\n",
    "        cumulative_rewards_log = []\n",
    "        win_log = []\n",
    "        epsilon_log = []\n",
    "        total_reward = 0.0  # For cumulative tracking\n",
    "        best_win_rate = 0.0\n",
    "        best_model_path = None\n",
    "        \n",
    "        for episode in range(self.n_episodes):\n",
    "            state, valid_moves_mask = self.env.reset()\n",
    "            state = state.view(-1).unsqueeze(0)         # shape: [1, state_size]\n",
    "            valid_moves_mask = valid_moves_mask.unsqueeze(0)  # shape: [1, action_size]\n",
    "\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.select_action(state, valid_moves_mask)\n",
    "                next_state, reward, done, valid_moves_mask = self.env.step(action)\n",
    "                next_state = next_state.view(-1).unsqueeze(0)\n",
    "                valid_moves_mask = valid_moves_mask.unsqueeze(0) # shape: [1, action_size]\n",
    "                #next_state_valid_moves_mask = valid_moves_mask  # shape: [1, action_size]\n",
    "\n",
    "                # Push to replay\n",
    "                agent.memory.push((state.cpu(), \n",
    "                                torch.tensor(action).cpu(), \n",
    "                                reward, \n",
    "                                next_state.cpu(),\n",
    "                                #next_state_valid_moves_mask.cpu(), \n",
    "                                done))\n",
    "\n",
    "                # Train\n",
    "                agent.train_step(self.batch_size)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            # Update target\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            # Logging\n",
    "            total_reward += episode_reward\n",
    "            rewards_log.append(episode_reward)\n",
    "            cumulative_rewards_log.append(total_reward)\n",
    "            win_log.append(1 if episode_reward > 0 else 0)\n",
    "            epsilon_log.append(agent.epsilon)\n",
    "\n",
    "\n",
    "            # Initialize variables for tracking the best cumulative reward and best win rate\n",
    "            best_cumulative_reward = float('-inf')\n",
    "            best_win_rate = float('-inf')\n",
    "            best_model_path = None  # To track the path of the best model\n",
    "\n",
    "            # Inside the loop:\n",
    "            if len(win_log) >= 100:\n",
    "                avg_win_rate = 100.0 * np.mean(win_log[-100:])\n",
    "                current_cumulative_reward = cumulative_rewards_log[-1] if cumulative_rewards_log else 0\n",
    "\n",
    "                # Save the model only if:\n",
    "                # 1. The new win rate is greater than the best win rate seen so far, or\n",
    "                # 2. The new win rate equals the best win rate, but the cumulative reward is higher\n",
    "                if (avg_win_rate > best_win_rate) or (avg_win_rate == best_win_rate and current_cumulative_reward > best_cumulative_reward):\n",
    "                    best_win_rate = avg_win_rate\n",
    "                    best_cumulative_reward = current_cumulative_reward\n",
    "\n",
    "                    # Save the model\n",
    "                    model_name = (\n",
    "                        f\"network_hl_{'_'.join(map(str, layer_structure))}_gamma_{gamma:.2f}_\"\n",
    "                        f\"bs_{self.batch_size}_tufq_{self.target_update_freq}_\"\n",
    "                        f\"wr_{int(best_win_rate)}_tr_{int(best_cumulative_reward)}.pth\"\n",
    "                    )\n",
    "                    best_model_path = os.path.join(models_dir, model_name)\n",
    "                    torch.save(agent.q_network.state_dict(), best_model_path)\n",
    "                    print(f\"New best model saved: {model_name}\")\n",
    "\n",
    "\n",
    "            # Print progress occasionally\n",
    "            if (episode+1) % self.log_interval == 0:  # Log interval\n",
    "                avg_reward = np.mean(rewards_log[-100:]) if len(rewards_log) > 100 else np.mean(rewards_log)\n",
    "                win_rate = 100.0 * np.mean(win_log[-100:]) if len(win_log) > 100 else 100.0 * np.mean(win_log)\n",
    "                print(f\"[{episode+1}/{self.n_episodes}] Layers={layer_structure}, Gamma={gamma}, \"\n",
    "                    f\"AvgReward(Last100)={avg_reward:.2f}, WinRate(Last100)={win_rate:.2f}%, Eps={agent.epsilon:.3f}\")\n",
    "\n",
    "        return rewards_log, cumulative_rewards_log, win_log, epsilon_log\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_experiments(self):\n",
    "        \n",
    "        # Centralized directory setup\n",
    "        save_dir = \"training_results_5x5_randomopponent_s'afteropponent_noattention\"\n",
    "        models_dir = os.path.join(save_dir, \"models\")\n",
    "        plots_dir = os.path.join(save_dir, \"plots\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        results = {}  # (layer_tuple, gamma) -> (rewards_log, cumulative_rewards_log, win_log, epsilon_log)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for gamma in self.gammas:\n",
    "                print(f\"=== Training with LayerStructure={layer}, Gamma={gamma}, Batch Size={self.batch_size}, Epsilon(max, min)={self.epsilon_max, self.epsilon_min}, mem_cap={self.memory_capacity}, Target Update={self.target_update_freq} ===\")\n",
    "                r_log, c_log, w_log, e_log = self.run_training(layer, gamma, models_dir)\n",
    "                results[(tuple(layer), gamma)] = (r_log, c_log, w_log, e_log)\n",
    "                \n",
    "                # Plot results for this combination\n",
    "                fig, axs = plt.subplots(3, 1, figsize=(16, 16))\n",
    "                \n",
    "                # Prepare parameter text\n",
    "                parameter_text = (\n",
    "                    f\"n_episodes={self.n_episodes}, max_t={self.max_t}, batch_size={self.batch_size}, \"\n",
    "                    f\"state_size={self.state_size}, action_size={self.action_size}, layers={layer}, \"\n",
    "                    f\"gamma={gamma:.2f}, epsilon_min={self.epsilon_min}, epsilon_max={self.epsilon_max}, \"\n",
    "                    f\"epsilon_decay={self.epsilon_decay}, memory_capacity={self.memory_capacity}, \"\n",
    "                    f\"device={self.device}, target_update_freq={self.target_update_freq}, lr={self.lr}, \"\n",
    "                    f\"log_interval={self.log_interval}\"\n",
    "                )\n",
    "                \n",
    "                # 1) Rewards\n",
    "                axs[0].plot(r_log, label=\"Rewards\")\n",
    "                rolling_avg_r = [np.mean(r_log[max(0, i-100):i+1]) for i in range(len(r_log))]\n",
    "                axs[0].plot(rolling_avg_r, label=\"Average Rewards (Last 100)\")\n",
    "                axs[0].set_xlabel(\"Episode\")\n",
    "                axs[0].set_ylabel(\"Reward\")\n",
    "                axs[0].set_title(f\"Rewards - Layers={layer}, Gamma={gamma}\", fontsize=14)\n",
    "                \n",
    "                axs[0].legend()\n",
    "                axs[0].grid()\n",
    "                \n",
    "                # 2) Cumulative Rewards\n",
    "                axs[1].plot(c_log, label=\"Cumulative Rewards\")\n",
    "                axs[1].set_xlabel(\"Episode\")\n",
    "                axs[1].set_ylabel(\"Total Reward\")\n",
    "                axs[1].set_title(f\"Cumulative Rewards - Layers={layer}, Gamma={gamma}\")\n",
    "                axs[1].legend()\n",
    "                axs[1].grid()\n",
    "                \n",
    "                # 3) Win Rate\n",
    "                rolling_win = [100.0*np.mean(w_log[max(0, i-100):i+1]) for i in range(len(w_log))]\n",
    "                axs[2].plot(rolling_win, label=\"Win Rate (Last 100 Episodes)\")\n",
    "                axs[2].set_xlabel(\"Episode\")\n",
    "                axs[2].set_ylabel(\"Win Rate (%)\")\n",
    "                axs[2].set_title(f\"Win Rate - Layers={layer}, Gamma={gamma}\")\n",
    "                axs[2].legend()\n",
    "                axs[2].grid()\n",
    "\n",
    "                fig.text(\n",
    "                    0.5, -0.15, parameter_text,  # Position below the figure\n",
    "                    ha='center', va='top', fontsize=9\n",
    "                )\n",
    "                \n",
    "                # Save the plot\n",
    "                plot_name = f\"network_hl_{'_'.join(map(str, layer))}_gamma_{gamma:.2f}_bs_{self.batch_size}_tufq_{self.target_update_freq}.png\"\n",
    "                plot_path = os.path.join(plots_dir, plot_name)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(plot_path, bbox_inches=\"tight\")  # Removes extra whitespace\n",
    "                plt.savefig(plot_path)\n",
    "                plt.show()\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training with LayerStructure=[125, 250, 250, 125], Gamma=0.68, Batch Size=64, Epsilon(max, min)=(1.0, 0.01), mem_cap=50000, Target Update=1000 ===\n",
      "[1000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.02, WinRate(Last100)=49.00%, Eps=0.692\n",
      "[2000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.12, WinRate(Last100)=44.00%, Eps=0.491\n",
      "[3000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.16, WinRate(Last100)=42.00%, Eps=0.365\n",
      "[4000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=0.00, WinRate(Last100)=50.00%, Eps=0.271\n",
      "[5000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.38, WinRate(Last100)=31.00%, Eps=0.195\n",
      "[6000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.22, WinRate(Last100)=39.00%, Eps=0.134\n",
      "[7000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=0.08, WinRate(Last100)=54.00%, Eps=0.090\n",
      "[8000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.48, WinRate(Last100)=26.00%, Eps=0.058\n",
      "[9000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.52, WinRate(Last100)=24.00%, Eps=0.039\n",
      "[10000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.40, WinRate(Last100)=30.00%, Eps=0.027\n",
      "[11000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.26, WinRate(Last100)=37.00%, Eps=0.020\n",
      "[12000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.54, WinRate(Last100)=23.00%, Eps=0.015\n",
      "[13000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.62, WinRate(Last100)=19.00%, Eps=0.012\n",
      "[14000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.60, WinRate(Last100)=20.00%, Eps=0.010\n",
      "[15000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.66, WinRate(Last100)=17.00%, Eps=0.010\n",
      "[16000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.56, WinRate(Last100)=22.00%, Eps=0.010\n",
      "[17000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.62, WinRate(Last100)=19.00%, Eps=0.010\n",
      "[18000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.58, WinRate(Last100)=21.00%, Eps=0.010\n",
      "[19000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.62, WinRate(Last100)=19.00%, Eps=0.010\n",
      "[20000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.68, WinRate(Last100)=16.00%, Eps=0.010\n",
      "[21000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.64, WinRate(Last100)=18.00%, Eps=0.010\n",
      "[22000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.56, WinRate(Last100)=22.00%, Eps=0.010\n",
      "[23000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.72, WinRate(Last100)=14.00%, Eps=0.010\n",
      "[24000/100000] Layers=[125, 250, 250, 125], Gamma=0.68, AvgReward(Last100)=-0.68, WinRate(Last100)=16.00%, Eps=0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m promising  \u001b[38;5;241m=\u001b[39m TrainAndPlot(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, \n\u001b[1;32m      2\u001b[0m                           max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, \n\u001b[1;32m      3\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                           lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m     15\u001b[0m                           log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpromising\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 162\u001b[0m, in \u001b[0;36mTrainAndPlot.run_experiments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gamma \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgammas:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Training with LayerStructure=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Gamma=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon(max, min)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_max,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mem_cap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_capacity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Target Update=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m     r_log, c_log, w_log, e_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     results[(\u001b[38;5;28mtuple\u001b[39m(layer), gamma)] \u001b[38;5;241m=\u001b[39m (r_log, c_log, w_log, e_log)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Plot results for this combination\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 94\u001b[0m, in \u001b[0;36mTrainAndPlot.run_training\u001b[0;34m(self, layer_structure, gamma, models_dir)\u001b[0m\n\u001b[1;32m     86\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush((state\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     87\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mtensor(action)\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     88\u001b[0m                 reward, \n\u001b[1;32m     89\u001b[0m                 next_state\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     90\u001b[0m                 \u001b[38;5;66;03m#next_state_valid_moves_mask.cpu(), \u001b[39;00m\n\u001b[1;32m     91\u001b[0m                 done))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     97\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[42], line 97\u001b[0m, in \u001b[0;36mDQNAgent.train_step\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()(q_values, target_q_values)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Decay epsilon\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactix-game-env/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactix-game-env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactix-game-env/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "promising  = TrainAndPlot(n_episodes=100000, \n",
    "                          max_t=1000, \n",
    "                          batch_size=64, \n",
    "                          state_size = 25, \n",
    "                          action_size = 125, \n",
    "                          layers = [[125, 250, 250, 125], [64, 128, 256, 128]], \n",
    "                          gammas = [0.68 , 0.8, 0.75], \n",
    "                          epsilon_min=0.01, \n",
    "                          epsilon_max=1.0, \n",
    "                          epsilon_decay=0.99995, \n",
    "                          memory_capacity=50000, \n",
    "                          device='cpu', \n",
    "                          target_update_freq=1000, \n",
    "                          lr=1e-4,\n",
    "                          log_interval=1000)\n",
    "promising.run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactix-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
